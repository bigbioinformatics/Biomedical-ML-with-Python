{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ace58e3-d3d7-4743-a14c-21a4f6e2aa21",
   "metadata": {},
   "source": [
    "# Module 6 - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c5bdf-2ad0-4cd2-89e5-c9b4a01b141b",
   "metadata": {},
   "source": [
    "Feature Selection is the process of choosing which features to use to answer your central question. Why would anyone want to limit the information availbale to them! Think Ockham's razor - when presented with competing hypotheses about the same prediction, one should select the solution with the fewest assumptions. In short - \"the simplest explination is usually the best one\". This concept of fugality applied to describing nature is what we call parsimony. In practice, we aim to develop models with the least number of features.\n",
    "\n",
    "The advantages to this are that models train faster, are less prone to overfitting, and are usually more accurate. In this excercise we will apply various feature selection schemes to the Mobile Price Classification dataset distributed with this notebook to examine how it effects model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c0e58-1d2a-4bf7-892a-dbc2f216828f",
   "metadata": {},
   "source": [
    " ## About the Mobile Price dataset \n",
    " 1. The data is already tidy and partitioned into training and testing csv files. \n",
    " 2. There are 2000 observations in the training set and 1000 in testing.\n",
    " 3. Each observation consisits of 20 phone features (columns) and one categorical label (final column) describing the phone's price range.\n",
    " 4. This is a classification problem. But for our case, it's an exercise in feature selection.\n",
    "\n",
    "### Data description\n",
    "| Feature | Description |\n",
    "| ------- | ----------- |\n",
    "| battery_power | Total energy a battery can store in one time measured in mAh |\n",
    "|blue | Has Bluetooth or not |\n",
    "|clock_speed | the speed at which microprocessor executes instructions |\n",
    "|dual_sim | Has dual sim support or not |\n",
    "| fc | Front Camera megapixels |\n",
    "| four_g | Has 4G or not |\n",
    "| int_memory | Internal Memory in Gigabytes |\n",
    "| m_dep | Mobile Depth in cm |\n",
    "| mobile_wt | Weight of mobile phone |\n",
    "| n_cores | Number of cores of the processor |\n",
    "| pc | Primary Camera megapixels |\n",
    "| px_height | Pixel Resolution Height |\n",
    "| px_width | Pixel Resolution Width |\n",
    "| ram | Random Access Memory in MegaBytes |\n",
    "| sc_h | Screen Height of mobile in cm |\n",
    "| sc_w | Screen Width of mobile in cm |\n",
    "| talk_time | the longest time that a single battery charge will last when you are |\n",
    "| three_g | Has 3G or not |\n",
    "| touch_screen | Has touch screen or not |\n",
    "| wifi | Has wifi or not |\n",
    "| price_range | This is the target variable with a value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103cd06",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's get all the requirements sorted before we move on to the excercise. Most packages should be familiar at this point. Numpy, pandas, matplotlib, and seaborn where all introduced in Part I of the workshop in modules 1-3 and last week in module 5 we introduced tableone. Notice, today we will be using sklearn for the first time to do some machine learning. Don't worry too much about the models we'll be using or how to train them for now. This will the the topic for modules 7 & 8.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "!pip install --upgrade ipykernel\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tableone\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "!pip install boruta\n",
    "\n",
    "# Globals\n",
    "seed = 1017\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tableone import TableOne\n",
    "from boruta import BorutaPy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#magic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f925e6",
   "metadata": {},
   "source": [
    "## What question am I answering?\n",
    "Well, we want to demonstrate the utility of feature selection. I think a convincing approach would be to compare predictive power in a model with and without feature selection. So, for every parsimonious model we train let's compare its performance with that of its couterpart prodigious model (i.e. model that uses all the features). Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bd5b1-61c0-435a-b1ae-3047d3519bae",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "As always we should have a look at how the features are distributed grouped by the labels. For this we'll generate a table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data as a pandas dataframe\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Generate table 1\n",
    "TableOne(df, groupby=df.columns[-1],\n",
    "         pval=True,\n",
    "         dip_test=True,\n",
    "         normal_test=True,\n",
    "         tukey_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ed772",
   "metadata": {},
   "source": [
    "## Comparing Models\n",
    "Let's define a function that will calculate the prodigious and parsimonious model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function that compares selected features to full model\n",
    "def compare_models(dataset, selfeat):\n",
    "    \"\"\"compare parsimonious and full linear model\"\"\"\n",
    "    \n",
    "    # get predictors and labels\n",
    "    X = dataset.drop('price_range',axis=1)  #independent columns\n",
    "    y = dataset['price_range']    #target column i.e price range\n",
    "\n",
    "    #get selected feature indecies\n",
    "    isel = [X.columns.get_loc(feat) for feat in selfeat if feat in X]\n",
    "    \n",
    "    #70-30 split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=seed)\n",
    " \n",
    "\n",
    "    #define the prodigious and parsimonious logistic models\n",
    "    prodmodel = LinearRegression()\n",
    "    parsmodel = LinearRegression()\n",
    "\n",
    "    #Fit the models\n",
    "    prodmodel.fit(X_train, y_train)\n",
    "    parsmodel.fit(X_train[selfeat], y_train) \n",
    "\n",
    "    #Report errors\n",
    "    display('Prodigious Model Score: %.2f' %prodmodel.score(X_test, y_test))\n",
    "    display('Parsimonious Model Score: %.2f' %parsmodel.score(X_test[selfeat], y_test))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667a1c1",
   "metadata": {},
   "source": [
    "## Filter Method\n",
    "The Table 1 conveniently has calculated the association of each feature with the outcome. Let's select only those features that are significatly (p<.05) associated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "selfeat = ['battery_power', 'int_memory', 'mobile_wt', 'px_height', 'px_width', 'ram', 'sc_h']\n",
    "compare_models(df, selfeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02c583",
   "metadata": {},
   "source": [
    "By keeping only 7 features the parsimonious model has the same score as the full model that uses all 20 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455bccb",
   "metadata": {},
   "source": [
    "## Usupervised Methods\n",
    "**Remove highly correlated features** To remove the correlated features, we can make use of the corr() method of the pandas dataframe. The corr() method returns a correlation matrix containing correlation between all the columns of the dataframe. A useful way to visualize the correlations is with a heatmap. We'll use the seaborn library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a correlation matrix for the columns in the dataset\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "#plot heat map\n",
    "plt.figure(figsize=(20,20))\n",
    "g=sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb8a65",
   "metadata": {},
   "source": [
    "We can loop through all the columns in the correlation_matrix and keep track of the features with a correlation value > 0.5. This 0.5 cut-off is quite strict and chosen for demonstration purposes. A more reasonable value is 80-90%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534fa397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init an empty set that will contain the names of the correlated features\n",
    "correlated_features = set()\n",
    "\n",
    "#loop over lower triangle of pairs of features\n",
    "#     do not consider the last feature which is the label \n",
    "for i in range(len(correlation_matrix .columns) - 1):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.5:\n",
    "            #accumulate the names of the second correlated feature\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            correlated_features.add(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the correlated features\n",
    "display(correlated_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a2287",
   "metadata": {},
   "source": [
    "These features are correlated to at least one other feature and can be considered redundant. Let's not include them in our parsimonious set and see how it effects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355abac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add label to the correlated features which we will drop\n",
    "correlated_features.add('price_range')\n",
    "selfeat = df.columns.drop(correlated_features)\n",
    "compare_models(df, selfeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833f61f",
   "metadata": {},
   "source": [
    "In this case the parsimonious model scores (goodness of fit) lower than the full model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5abb7",
   "metadata": {},
   "source": [
    "## Wrapper Methods\n",
    "**Recursive feature elimination (RFE)** is a stepwise feature selection process implemented in sklearn. Recall, the model used for feature selection does not have to be the same as the predictive model. Here we will use a tree based model for RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictors and labels\n",
    "X = df.drop('price_range', axis=1)  \n",
    "y = df['price_range']\n",
    "\n",
    "# use tree based model for RFE\n",
    "rfe = RFECV(estimator=DecisionTreeClassifier())\n",
    "\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# summarize all features\n",
    "for i in range(X.shape[1]):\n",
    "    display('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7164f",
   "metadata": {},
   "source": [
    "We can see which features were selected by thier column index. They correspond to features 'battery_power', 'px_height', 'px_width', and 'ram' . Let's compare the parsimonious linear model with the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b2f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the column indecies\n",
    "selcol = [0, 11, 12, 13]\n",
    "#get the column names\n",
    "selfeat = df.columns[selcol]\n",
    "#compare models\n",
    "compare_models(df, selfeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ec102",
   "metadata": {},
   "source": [
    "**Boruta** is another wrapper method I like to use. It can be faster than RFE as the number of features increases and stands on a more solid statistical footing. To improve RFE statistics one could employ a repeated k-fold cross vaildation scheme but that would increase the computation time even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictors and labels\n",
    "X = np.array(df.drop('price_range', axis=1)) \n",
    "y = np.array(df['price_range'])\n",
    "\n",
    "# define random forest classifier for boruta\n",
    "forest = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "forest.fit(X, y)\n",
    "\n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(forest, n_estimators='auto', verbose=0, random_state=seed)\n",
    "\n",
    "# find all relevant features\n",
    "feat_selector.fit(X, y)\n",
    "\n",
    "# zip my names, ranks, and decisions in a single iterable\n",
    "feature_ranks = list(zip(df.columns.drop('price_range'), \n",
    "                         feat_selector.ranking_, \n",
    "                         feat_selector.support_))\n",
    "\n",
    "# iterate through and print out the results\n",
    "for feat in feature_ranks:\n",
    "    display('Feature: {:<25} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5249a03",
   "metadata": {},
   "source": [
    "Looks like bortua selected battery_power, px_height, px_width, and ram. These are the same features selected by RFE so we'll move on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246afc55",
   "metadata": {},
   "source": [
    "## Embedded methods\n",
    "**LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de19942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# get predictors and labels\n",
    "X = np.array(df.drop('price_range', axis=1)) \n",
    "y = np.array(df['price_range'])\n",
    "\n",
    "#train lasso model with 5-fold cross validataion\n",
    "lasso = LassoCV(cv=5, random_state=0).fit(X, y)\n",
    "\n",
    "#display the model score\n",
    "lasso.score(X, y)\n",
    "\n",
    "#plot feature importance based on coeficients\n",
    "importance = np.abs(lasso.coef_)\n",
    "feature_names = np.array(df.columns.drop('price_range'))\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2556afc4",
   "metadata": {},
   "source": [
    "Again we see battery power, px_height, px_width, and ram are the most important features that influence price.\n",
    "\n",
    "## Conclusions\n",
    "I hope I have given you a fair overview of different feature selection schemes. Notice, I have not used the testing set to validate any relationships we have found. The next step would be to aggregate the information you have gained from the various feature selection schemes and use them to decide which features to include in your final model.  Also, Notice there were some warnings raised by the table 1 when we first loaded the data. Addressing these errors could improve your final model's performance; remember garbage in garbage out. I'll leave that as an excercise to you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
