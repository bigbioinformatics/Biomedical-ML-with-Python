{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db80c954",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "Last week we introduced the [challenging Acute Myeloid Leukemia dataset](https://www.synapse.org/#!Synapse:syn2455683/wiki/64007) to test your ML skills. Let us take a few minutes before we begin today to touch base. To recap we indicated that you will have to request access to this data through the link provided and follow directions to the 'request access' form. At this point you should ideally have the data and have begun formatting.\n",
    "\n",
    "Last week we outlined the analysis as follows.\n",
    "+ ***Format*** the data with observations in rows and features in columns\n",
    "+ ***Manually exclude data*** like book keeping variables\n",
    "+ ***Normalize*** Data\n",
    "+ ***Treat missing*** Data\n",
    "+ Choose and employ a ***Data Partitioning*** scheme\n",
    "+ Do ***Feature selection*** for Clinical covariates and ***Dimension Reduction*** for omics data.\n",
    "+ ***Choose a model*** with the highest performance potential\n",
    "+ ***Tune your model***\n",
    "+ Caclulate ***Performance metrics***\n",
    "+ Report ***Model Predicitions***\n",
    "+ ***Interpret*** results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0d32a",
   "metadata": {},
   "source": [
    "## Synapse API\n",
    "The `synapseclient` package provides an interface to [Synapse](http://www.synapse.org/), a collaborative workspace for reproducible data intensive research projects, providing support for:\n",
    "\n",
    " + integrated presentation of data, code and text\n",
    "\n",
    " + fine grained access control\n",
    "\n",
    " + [provenance tracking](https://python-docs.synapse.org/build/html/Activity.html)\n",
    "\n",
    "The `synapseclient` package lets you communicate with the cloud-hosted Synapse service to access data and create shared data analysis projects from within Python scripts or at the interactive Python console. Other Synapse clients exist for [R](https://r-docs.synapse.org/), [Java](https://github.com/Sage-Bionetworks/Synapse-Repository-Services/tree/develop/client/synapseJavaClient), and the [web](https://www.synapse.org/). The Python client can also be used from the [command line](https://python-docs.synapse.org/build/html/CommandLineClient.html).\n",
    "\n",
    "If you’re just getting started with Synapse, have a look at the Getting Started guides for [Synapse](https://docs.synapse.org/articles/getting_started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade synapseclient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870434c4",
   "metadata": {},
   "source": [
    "### Connecting to Synapse\n",
    "To use Synapse, you’ll need to [register](https://www.synapse.org/register) for an account. The Synapse website can authenticate using a Google account, but you’ll need to take the extra step of creating a Synapse password to use the programmatic clients.\n",
    "\n",
    "Once that’s done, you’ll be able to load the library, create a [Synapse](https://python-docs.synapse.org/build/html/index.html#synapseclient.Synapse) object and login:\n",
    "\n",
    "Valid combinations of login() arguments:\n",
    "+ email/username and password\n",
    "+ email/username and apiKey (Base64 encoded string)\n",
    "+ authToken\n",
    "+ sessionToken (DEPRECATED)\n",
    "\n",
    "If no login arguments are provided or only username is provided, login() will attempt to log in using\n",
    "information from these sources (in order of preference):\n",
    "1. User’s personal access token from environment the variable: SYNAPSE_AUTH_TOKEN\n",
    "2. `.synapseConfig` file (in user home folder unless configured otherwise)\n",
    "3. cached credentials from previous login() where rememberMe=True was passed as a parameter\n",
    "\n",
    "\n",
    "### Login the safe way\n",
    "The safest login method utilizes an api key stored locally in your home directory. You can get your apikey from the account settings on your Synapse dashboard. A copy of the default .synapseConfig template can also be obtained [here](https://raw.githubusercontent.com/Sage-Bionetworks/synapsePythonClient/develop/synapseclient/.synapseConfig).\n",
    "\n",
    "\n",
    "### Login the easy way\n",
    "The easiest way to login is to enter your email and password in plain text **yikes!!** and use the `rememberMe=True` option. This will cache your credentials on your local machine. Then, immediately remove your plain text credentials from the notebook. Next time you run the notebook it should look up your saved credentials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapseclient\n",
    "\n",
    "#create synapse object\n",
    "syn = synapseclient.Synapse()\n",
    "\n",
    "#login the easy\n",
    "#syn.login(email='my_username', password='my_password', rememberMe=True)\n",
    "\n",
    "#login the safe way\n",
    "#syn.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3235f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if my credentials were saved properly\n",
    "syn.logout()\n",
    "syn.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbaa921",
   "metadata": {},
   "source": [
    "### Data Download  & Caching\n",
    "Files can be downloaded in bulk using the syncFromSynapse function found in the synapseutils helper package. This function crawls all the subfolders of the project or folder that you specify and retrieves all the files that have not been downloaded. By default, the files will be downloaded into your synapseCache, but a different download location can be specified with the path parameter. If you do download to a location out side of synapseCache, this function will also create a tab-delimited manifest of all the files along with their metadata (path, provenance, annotations, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapseutils\n",
    "\n",
    "# download Leukemia datasets from Synapse to the data folder distributed with this notebook\n",
    "files = synapseutils.syncFromSynapse(syn, entity='syn2488690', path='./data') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d888afd",
   "metadata": {},
   "source": [
    "You'll find 4 files downloaded to the data folder distributed with this notebook.\n",
    "+ scoringData-extendedChallenge-noChemo-release.xlsx\n",
    "+ trainingData-extendedChallenge-noChemo-release.xlsx\n",
    "+ scoringData-release.csv\n",
    "+ trainingData-release.csv\n",
    "\n",
    "and a manifest file\n",
    "+ SYNAPSE_METADATA_MANIFEST.tsv\n",
    "containing the downloaded file paths and some other metadata not important for us now.\n",
    "\n",
    "We will be working with the `trainingData-release.csv` and `scoringData-release.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada45389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data as pandas dataframe and display head\n",
    "df = pd.read_csv('data/trainingData-release.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ace58e3-d3d7-4743-a14c-21a4f6e2aa21",
   "metadata": {},
   "source": [
    "# Module 10 - Leukemia project  week 2\n",
    "\n",
    "Today we will complete the kaggle Leukemia in class analysis. Continue on your own with the Synapse Dream Challenge Leukemia dataset analysis. \n",
    "\n",
    "\n",
    "We will be extending last week's notebook where we processed the gene expression data uploaded from kaggle. To quickly recap the kaggle dataset was aquired in 1999 and later [published in Science](https://doi.org/10.1126/science.286.5439.531). The paper demonstrated how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. The data was used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). Our excersice is to develop a model that discriminates between AML and ALL patients based only on the this gene expression data and compare our model with the published results of the 1999 paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103cd06",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's get all the requirements sorted before we move on to the excercise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fec48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "!pip install --upgrade ipykernel\n",
    "!pip install kaggle\n",
    "!pip install pandas\n",
    "!pip install tableone\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install seaborn\n",
    "\n",
    "# Globals\n",
    "seed = 1017\n",
    "\n",
    "#imports\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tableone import TableOne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "#magic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040877a2",
   "metadata": {},
   "source": [
    "## Loading the data via kaggle API\n",
    "The leukemia data set was sourced from kaggle. \n",
    "To download the data directly from [kaggle](https://kaggle.com) you will need to have a kaggle account. **It's free.** Once you create your kaggle account you can generate an API token. After you log in you should see a circular account icon in the upper-right of any kaggle page. Clicking on your account icon will open a right-sidebar where you can select \"Account\" to edit your account. Scroll down to the API section and click on the \"create new api token\" button. An API token should automatically download and a prompt will also appear telling you which directory to put this token so python knows where find it. For MacOS users this location is \"~/.kaggle/kaggle.json\". Once you have done this modify the code below to download the dataset to the `data` folder distributed with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log in to kaggle using your api token\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "#path relative to this notebook to put the data\n",
    "datadir = 'data'\n",
    "\n",
    "#name of the dataset on kaggle\n",
    "dataset = 'crawford/gene-expression'\n",
    "\n",
    "#downlaod the data\n",
    "kaggle.api.dataset_download_files(dataset, path=datadir, unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bd5b1-61c0-435a-b1ae-3047d3519bae",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "There are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d23b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data as a pandas dataframe\n",
    "labels = pd.read_csv('data/actual.csv', index_col = 'patient')\n",
    "test = pd.read_csv('data/data_set_ALL_AML_independent.csv')\n",
    "train = pd.read_csv('data/data_set_ALL_AML_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366f740",
   "metadata": {},
   "source": [
    "***Task*** Use the head() function to quickly have a look at the training data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faeb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the head() function display th first few rows of the training data frame.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6cf3b",
   "metadata": {},
   "source": [
    "The testing set is formatted the same as the training set. Notice, that the gene description and accession numbers are given along with the count and outcome (call) for each patient. The patient outcomes are also provided in the file `actual.csv`. I think it will be more convientient to use the outcomes in this file and delete the 'call' columns in both the training and testing sets. ***Question: What are the observational units of interest?*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60583afe",
   "metadata": {},
   "source": [
    "## Formatting\n",
    "***Task*** Remove the 'call' columns from the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--remove this for student verssion--> \n",
    "cols = [col for col in test.columns if 'call' in col]\n",
    "test = test.drop(cols, 1)\n",
    "\n",
    "cols = [col for col in train.columns if 'call' in col]\n",
    "train = train.drop(cols, 1)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97cd6a",
   "metadata": {},
   "source": [
    "Let's consider what the observational unit should be. ***Task*** Format the data to have observations in rows and features in columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this for stutent version \n",
    "train = train.T\n",
    "test = test.T\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f645b",
   "metadata": {},
   "source": [
    "We can also remove the gene bookkeeping data. However, it's probably a good idea to make a copy of the Gene Accession Numbers for later analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41caa04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Gene Accession Numbers\n",
    "accnum = train[:2]\n",
    "accnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the gene bookkeeping data.\n",
    "train = train.drop([\"Gene Description\", \"Gene Accession Number\"])\n",
    "test = test.drop([\"Gene Description\", \"Gene Accession Number\"])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954102d7",
   "metadata": {},
   "source": [
    "Now let's encode the outcomes for binary classification. We'll use Zeros for the ALL outcomes and Ones for AML. Remember the first 38 patients were partitioned for the training set the remainder are in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.replace({'ALL':0, 'AML':1})\n",
    "labels_train = labels[0:38]\n",
    "labels_test = labels[38:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caf6ea",
   "metadata": {},
   "source": [
    "### Treat missing data\n",
    "Before moving on to a table 1. Let's look for and treat any missing data this. Remember to check for values that don't make sense. I think replacing witht the mean value would be a reasonable imputation strategy.***Task*** check for unreasonable and missing values and impute them with the column mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove inf\n",
    "train = train.replace( np.inf , np.nan )\n",
    "test = test.replace( np.inf , np.nan )\n",
    "\n",
    "#impute with mean\n",
    "train = train.fillna(value = train.values.mean())\n",
    "test = test.fillna(value = train.values.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74577963",
   "metadata": {},
   "source": [
    "***Question*** How would you go about visualizing the data we just formatted? Why can't I just make a table 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debb21d",
   "metadata": {},
   "source": [
    "### PCA for Dimension Reduction\n",
    "To motivate the concept, let's project the ~7k dimesions to the first 10 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load preprocessing and PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Do a PCA for all data (this should probably be done only with the training data)\n",
    "nPC=10\n",
    "pca = PCA(n_components = nPC, random_state = seed)\n",
    "\n",
    "#define standard scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#fit pca on the training data\n",
    "X_scaled = scaler.fit_transform(train)\n",
    "X_train_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "#transform the testing data using the fit scaler and pca objects\n",
    "X_test_pca = pca.transform(scaler.transform(test))\n",
    "\n",
    "print(X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044a923",
   "metadata": {},
   "source": [
    "---\n",
    "**Question:** How many PCs are needed to explain 80% of the variance in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load preprocessing and PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Full dof PCA for all data\n",
    "pca = PCA(random_state = seed) #do not define number of PCs\n",
    "\n",
    "#define standard scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#fit pca on the training data\n",
    "X_scaled = scaler.fit_transform(train)\n",
    "X_train_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# compute total variation explained\n",
    "total = sum(pca.explained_variance_)\n",
    "\n",
    "# init number of PCs to use\n",
    "nPC = 0\n",
    "\n",
    "#init variance explained using 0 PCs\n",
    "cum_variance = 0\n",
    "\n",
    "#loop while cumulative variance explained is less than 80% of the total\n",
    "<your code>\n",
    "\n",
    "#display number of PC chosen\n",
    "print(nPC, \" features explain 80% of the variance. Suggested dimesnion reduction of 7129 features to \", nPC, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do a PCA with selected number of PCs\n",
    "pca = PCA(n_components = nPC, random_state = seed)\n",
    "\n",
    "#fit pca on the training data\n",
    "X_train_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "#transform the testing data using the fit scaler and pca objects\n",
    "X_test_pca = pca.transform(scaler.transform(test))\n",
    "\n",
    "print(X_train_pca.shape)\n",
    "\n",
    "#plot ratio of variance explained by nPCs\n",
    "var_exp = pca.explained_variance_ratio_.cumsum()\n",
    "var_exp = var_exp*100\n",
    "plt.bar(range(1, nPC + 1), var_exp, color=\"brown\")\n",
    "plt.xlabel(\"Number of components\", fontsize=18)\n",
    "plt.ylabel(\"Cumulutive variance explained\", fontsize=18)\n",
    "plt.xlim((0.5, nPC + 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d5a51",
   "metadata": {},
   "source": [
    "### Visualize Engineered Features\n",
    "Let's plot the feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b796e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the engineered features\n",
    "X_train_pca = scaler.fit_transform(X_train_pca)\n",
    "X_test_pca = scaler.transform(X_test_pca)\n",
    "\n",
    "#plot the principle component distributions\n",
    "for PC in range(nPC):\n",
    "    sns.kdeplot(data=X_train_pca[:, PC])\n",
    "    #X_pca[:,nPC].plot.kde(bw_method='scott') #use bw_method=.02 for a lower bandwidth gaussian representation\n",
    "    plt.legend([\"PC\" + str(PC)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4d9f9",
   "metadata": {},
   "source": [
    "## LASSO for feature selection\n",
    "\n",
    "Let's use LASSO to select which of these engineered features to use in our model.\n",
    "\n",
    "**Question:** Why would LASSO be a good choice for these engineered features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitioning disco-train & testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get predictors and labels (ideally use 30% disco set)\n",
    "X = X_train_pca\n",
    "y = np.array(labels_train).squeeze()\n",
    "\n",
    "#split 30% of the the training data off for discovery\n",
    "X_train, X_disco, y_train, y_disco = train_test_split(X, y, test_size=.3, random_state=seed, shuffle=True, stratify=y)\n",
    "\n",
    "#get the testing set predictors and labels\n",
    "X_test = X_test_pca \n",
    "y_test = np.array(labels_test).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76cf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "#train lasso model with 5-fold cross validataion using disco set\n",
    "lasso = <your code>\n",
    "\n",
    "#plot feature importance based on coeficients\n",
    "importance = np.abs(lasso.coef_)\n",
    "feature_names = np.array(range(X_disco.shape[1]))\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()\n",
    "\n",
    "#select PCs with non-zero weight\n",
    "sel_features = feature_names[importance>0]\n",
    "print(sel_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dca6ab",
   "metadata": {},
   "source": [
    "## Benchmark model Logistic Regression\n",
    "We'll use logistic regression as our benchmark model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Logistic Regression model and metric\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "#define logistic regresssion model and train with training set\n",
    "benchmark = <your code>\n",
    "\n",
    "#display performance metrics\n",
    "display(\"accuracy = \" + str(benchmark.score(X_test, y_test)))\n",
    "display(\"f1 score = \" + str(f1_score(y_test, benchmark.predict(X_test))))\n",
    "display(confusion_matrix(y_test, benchmark.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a017ce",
   "metadata": {},
   "source": [
    "\n",
    "## Train Stacked model\n",
    "Let's train a panel of mesa models and use thier predictions to train a meta model. For this we will use the output of a RF, KNN and support vector machine inputs into a logistic regression meta model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#define mesa models\n",
    "estimators = [('rf', <your rf classifier>),\n",
    "              ('knn', <your kn classifier>),\n",
    "              ('svm', <your svm classifier>)\n",
    "             ]\n",
    "\n",
    "#stack mesa models on a logistic regression meta model\n",
    "stakt = StackingClassifier(estimators=estimators,\n",
    "                           final_estimator=<your log reg classifier>)\n",
    "\n",
    "#train stacked model\n",
    "stakt.fit(X_train, y_train)\n",
    "\n",
    "#display performance metrics\n",
    "display(\"accuracy = \" + str(benchmark.score(X_test, y_test)))\n",
    "display(\"f1 score = \" + str(f1_score(y_test, benchmark.predict(X_test))))\n",
    "display(confusion_matrix(y_test, benchmark.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a500cb",
   "metadata": {},
   "source": [
    "## Compare ROCs using delongs test.\n",
    "DeLong is an asymptotically exact method to evaluate the uncertainty of an AUC ([DeLong et al. (1988)](https://www.jstor.org/stable/2531595). It can be used to compare if two AUC's are statistically different. Let's plot the ROCs of the benchmark and stacked models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#get Benchmark class probabilities\n",
    "preds = benchmark.predict_proba(X_test)[:,1]\n",
    "\n",
    "#compute ROC curve and AUC\n",
    "fpr, tpr, _ = <call roc_curve function>\n",
    "roc_auc = <call auc function>\n",
    "\n",
    "#plot benchmark ROC curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Benchmark receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88738ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#get Model class probabilities\n",
    "preds = stakt.predict_proba(X_test)[:,1]\n",
    "\n",
    "#compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#plot Model ROC curve\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Stacked Model receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2673486",
   "metadata": {},
   "source": [
    "Fortunately, Yandex Data School has a Fast DeLong implementation on their public repo:\n",
    "https://github.com/yandexdataschool/roc_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy of deLong's test calculator from Yandex Data School\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Operating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=float)\n",
    "    ty = np.empty([k, n], dtype=float)\n",
    "    tz = np.empty([k, m + n], dtype=float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    return order, label_1_count\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "def delong_roc_test(ground_truth, predictions_one, predictions_two):\n",
    "    \"\"\"\n",
    "    Computes log(p-value) for hypothesis that two ROC AUCs are different\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions_one: predictions of the first model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "       predictions_two: predictions of the second model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = np.vstack((predictions_one, predictions_two))[:, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    return calc_pvalue(aucs, delongcov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine if the benchmark and stacked model AUCs are statistically different\n",
    "\n",
    "#get ground truth labels\n",
    "ground_truth = y_test\n",
    "\n",
    "#get class probabilities\n",
    "predictions_one = benchmark.predict_proba(X_test)[:,1]\n",
    "predictions_two = stakt.predict_proba(X_test)[:,1]\n",
    "\n",
    "#test if two predictions are different\n",
    "pval = <call delong roc test function>\n",
    "\n",
    "display(\"DeLong's AUC test pvalue =\" + str(pval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e9757",
   "metadata": {},
   "source": [
    "We can also calculate the confidence intervals on the AUC of the two models to demonstrate if they are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd81af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define significance\n",
    "alpha = .95\n",
    "\n",
    "#get bechmark stats\n",
    "auc, auc_cov = <call delong_roc_variance fucntion>\n",
    "\n",
    "#compute confidence interval\n",
    "auc_std = np.sqrt(auc_cov)\n",
    "lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "ci = stats.norm.ppf(\n",
    "    lower_upper_q,\n",
    "    loc=auc,\n",
    "    scale=auc_std)\n",
    "\n",
    "ci[ci > 1] = 1\n",
    "\n",
    "print('Benchmark AUC:', auc)\n",
    "print('Benchmark AUC COV:', auc_cov)\n",
    "print('Benchmark 95% AUC CI:', ci)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat CI calculation for stacked model stats  \n",
    "<your code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9129761",
   "metadata": {},
   "source": [
    "## Interpretations\n",
    "\n",
    "**Question**: What does it mean if model AUC is the same as that of the benchmark?\n",
    "\n",
    "**Question**: Does this analysis justify using a more complicated model in a future study that goes beyond logistic regression?\n",
    "\n",
    "**Question**: What statement about the biology can you make with this analysis?\n",
    "\n",
    "**Question**: How would you have designed the analysis to improve the AUC?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
